{"cells":[{"cell_type":"markdown","source":["##목적 : 단어를 쓰면 가사가 출력되는 모델을 만들어보자"],"metadata":{"id":"1xOFZYpYLiRz"}},{"cell_type":"markdown","metadata":{"id":"ossi_MoH14tD"},"source":["- 데이터 불러오기"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3019,"status":"ok","timestamp":1660009816925,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"36pTdYbkxyVk","outputId":"58d80ec3-96b8-46b2-96f3-bd0819e7d6a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 크기: 187088\n","Examples:\n"," ['I. LIFE.', '', '']\n"]}],"source":["import glob\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","txt_file_path = '/content/drive/MyDrive/AIFFEL/E-06/data_set/lyrics/*'\n","\n","txt_list = glob.glob(txt_file_path)\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"]},{"cell_type":"markdown","metadata":{"id":"qJNvzme2161m"},"source":["- 데이터 정제"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1660009816926,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"efEsOegQ6-M6","outputId":"3dddff11-4c40-407e-eee8-95ebd1ddb6d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["I. LIFE.\n","I.\n","SUCCESS.\n"]}],"source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n","\n","    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n","        \n","    print(sentence)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1660009816927,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"3HUJfySS1qVo"},"outputs":[],"source":["# 입력된 문장을\n","#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","#     2. 특수문자 양쪽에 공백을 넣고\n","#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","#     5. 다시 양쪽 공백을 지웁니다\n","#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5\n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2780,"status":"ok","timestamp":1660009819701,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"yy8O1Bxt2lUZ","outputId":"424a1d2a-803d-4b3f-bdf0-e737e4c9b05a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> i . life . <end>',\n"," '<start> i . <end>',\n"," '<start> success . <end>',\n"," '<start> published in a masque of poets <end>',\n"," '<start> at the request of h . h . , the author s <end>',\n"," '<start> fellow townswoman and friend . <end>',\n"," '<start> success is counted sweetest <end>',\n"," '<start> by those who ne er succeed . <end>',\n"," '<start> to comprehend a nectar <end>',\n"," '<start> requires sorest need . <end>']"]},"metadata":{},"execution_count":4}],"source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","\n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"]},{"cell_type":"markdown","source":["- 평가 데이터셋 분리"],"metadata":{"id":"fMvITsvcO6nk"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6067,"status":"ok","timestamp":1660009825765,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"eu7pNiCs3NmL","outputId":"ff34f791-dff0-4a94-fad0-9bf100c8ada4"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[   2    5   20 ...    0    0    0]\n"," [   2    5   20 ...    0    0    0]\n"," [   2 2762   20 ...    0    0    0]\n"," ...\n"," [ 130    5   22 ...   10 1019    3]\n"," [   5   37   15 ...  876  644    3]\n"," [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fcbfabac050>\n"]}],"source":["# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n","def tokenize(corpus):\n","  # 12000단어를 기억할 수 있는 tokenizer를 만들꺼임\n","  # 우린는 이미 문장은 정제했으니 filter가 필요 x\n","  # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿈\n","  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      num_words=12000,\n","      filters=' ',\n","      oov_token='<unk>'\n","  )\n","  # corpus를 이용해 tokenizer 내부의 단어장을 완성\n","  tokenizer.fit_on_texts(corpus)\n","  # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n","  tensor = tokenizer.texts_to_sequences(corpus)\n","  # 입력 데이터의 시퀀스 길이를 일정하게 맞춤\n","  # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춤\n","  # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n","  # maxlen을 16으로 해서 과도한 padding 방지\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=16)\n","\n","  print(tensor, tokenizer)\n","  return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"]},{"cell_type":"code","source":["# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력\n","print(tensor[:3, :10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pAi6H68cM9Rx","executionInfo":{"status":"ok","timestamp":1660009825766,"user_tz":-540,"elapsed":107,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"6ab22ae0-a4d2-4ce4-f530-a01d7288fe53"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[[   2    5   20  102   20    3    0    0    0    0]\n"," [   2    5   20    3    0    0    0    0    0    0]\n"," [   2 2762   20    3    0    0    0    0    0    0]]\n"]}]},{"cell_type":"code","source":["# 텐서 데이터는 모두 정수로 이루어짐\n","# 이 숫자는 tokenizer에 구축된 단어 사전 인덱스\n","# 단어 사전이 어떻게 구성되었는지 확인\n","for idx in tokenizer.index_word:\n","  print(idx, ':', tokenizer.index_word[idx])\n","\n","  if idx >= 10: break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVcMp9ZZNLtp","executionInfo":{"status":"ok","timestamp":1660009825768,"user_tz":-540,"elapsed":32,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"fdd71fc6-ad25-4c22-abbc-a45cfabaed8f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : i\n","6 : the\n","7 : you\n","8 : and\n","9 : a\n","10 : to\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1660009825769,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"g5c2FYAM7eg3","outputId":"5dfb90b5-06b3-4d06-df44-6bde46790b1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[  2   5  20 102  20   3   0   0   0   0   0   0   0   0   0]\n","[  5  20 102  20   3   0   0   0   0   0   0   0   0   0   0]\n"]}],"source":["from IPython.lib.display import ScribdDocument\n","# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n","src_input = tensor[:, :-1]\n","# tensor에서 <start>를 잘라내 타겟 문장 생성\n","tgt_input = tensor[:, 1:]\n","\n","print(src_input[0])\n","print(tgt_input[0])"]},{"cell_type":"code","source":["#  데이터셋 객체를 생성\n","BUFFER_SIZE = len(src_input)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(src_input) // BATCH_SIZE\n","# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE = tokenizer.num_words + 1\n","\n","# 준비한 데이터 소스로부터 데이터 셋을 만듦\n","dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOb2VW19Nmbd","executionInfo":{"status":"ok","timestamp":1660009826870,"user_tz":-540,"elapsed":361,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"cffd499b-0286-42ee-c089-e7dd4954f1a4"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(256, 15), dtype=tf.int32, name=None), TensorSpec(shape=(256, 15), dtype=tf.int32, name=None))>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1660009826871,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"PuqppCY2889k"},"outputs":[],"source":["# 훈련 데이터와 평가 데이터를 분리\n","enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=7)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1660009826872,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"R0MMnanopwiL","outputId":"2e7a95ca-8ed1-44bf-80fd-20d0be397326"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(140788, 15)"]},"metadata":{},"execution_count":11}],"source":["# 훈련 데이터 확인\n","enc_train.shape"]},{"cell_type":"markdown","source":["- 인공지능 학습시키기"],"metadata":{"id":"97h_-csuNQSc"}},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1660009827202,"user":{"displayName":"곽민기","userId":"11982204351407286297"},"user_tz":-540},"id":"Hv58y2h09duE"},"outputs":[],"source":["# 모델 학습 준비\n","class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 2048\n","hidden_size = 2048\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"]},{"cell_type":"code","source":["# 데이터셋에서 데이터 한 배치만 불러오는 방법\n","for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsmNKmDcNamC","executionInfo":{"status":"ok","timestamp":1660009845754,"user_tz":-540,"elapsed":18554,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"389934fc-b61b-4986-f253-750a55681bb4"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 15, 12001), dtype=float32, numpy=\n","array([[[-6.5123494e-04, -1.6956308e-04, -4.7167705e-04, ...,\n","          4.9701188e-04, -2.0004326e-04,  1.2820296e-04],\n","        [-6.9413771e-04, -7.4705982e-04, -3.3177590e-04, ...,\n","          1.0930588e-03, -4.2499229e-04,  2.9746635e-04],\n","        [-3.1701796e-04, -4.9526471e-04, -4.6928591e-04, ...,\n","          4.9289630e-04, -8.0005592e-04,  4.2378527e-04],\n","        ...,\n","        [-8.7332487e-04,  3.8362251e-04, -2.2249192e-03, ...,\n","          4.3192105e-03,  5.7042803e-04, -1.9135049e-03],\n","        [-2.9435314e-04,  2.9590441e-04, -2.6041986e-03, ...,\n","          4.3828301e-03,  8.5326860e-04, -1.9833366e-03],\n","        [-5.0612155e-04,  4.9903040e-04, -2.3827278e-03, ...,\n","          4.7529074e-03,  1.2391321e-03, -1.7811675e-03]],\n","\n","       [[-6.5123494e-04, -1.6956308e-04, -4.7167705e-04, ...,\n","          4.9701188e-04, -2.0004326e-04,  1.2820296e-04],\n","        [-1.5486372e-03, -5.5365279e-05, -1.1753297e-03, ...,\n","          4.9253390e-04, -1.7784326e-04,  3.8464231e-04],\n","        [-1.9824838e-03,  8.6865359e-05, -1.7249375e-03, ...,\n","          5.3252658e-04, -6.3502567e-04,  1.9238444e-04],\n","        ...,\n","        [ 3.0752702e-03,  2.6173427e-04, -1.0884614e-03, ...,\n","         -3.6098098e-03,  8.7389461e-04, -6.0851622e-04],\n","        [ 3.3920426e-03, -2.9455021e-04, -1.5055763e-03, ...,\n","         -4.1388902e-03,  1.8546637e-03, -1.4623392e-03],\n","        [ 3.6509491e-03, -8.9037640e-04, -1.9189341e-03, ...,\n","         -4.6585994e-03,  2.8074239e-03, -2.3241916e-03]],\n","\n","       [[-6.5123494e-04, -1.6956308e-04, -4.7167705e-04, ...,\n","          4.9701188e-04, -2.0004326e-04,  1.2820296e-04],\n","        [-6.4242608e-04, -1.6318154e-04, -9.8055264e-04, ...,\n","         -2.2587963e-04, -7.7323936e-04,  5.1708118e-04],\n","        [-8.6925848e-04, -3.8367699e-04, -1.0676758e-03, ...,\n","         -3.0614101e-04, -7.0904894e-04,  1.3387481e-03],\n","        ...,\n","        [ 3.6075764e-04, -1.4110440e-03, -1.7120965e-03, ...,\n","         -4.1868334e-05,  5.5325567e-04,  2.9268626e-03],\n","        [ 1.0816262e-03, -1.8006599e-03, -2.2215655e-03, ...,\n","         -7.1386679e-04,  1.2970641e-03,  2.1186040e-03],\n","        [ 1.7407365e-03, -2.2637809e-03, -2.6578463e-03, ...,\n","         -1.4538727e-03,  2.1204296e-03,  1.1833594e-03]],\n","\n","       ...,\n","\n","       [[-6.5123494e-04, -1.6956308e-04, -4.7167705e-04, ...,\n","          4.9701188e-04, -2.0004326e-04,  1.2820296e-04],\n","        [-1.2404773e-03, -1.2768485e-04, -1.3334141e-03, ...,\n","          3.4393769e-04, -1.2984192e-03,  3.7166352e-05],\n","        [-1.9893853e-03,  1.9733231e-04, -1.6217516e-03, ...,\n","          5.1665120e-04, -1.7843974e-03,  1.0359749e-04],\n","        ...,\n","        [-2.5785537e-03,  2.5545652e-03, -5.0428120e-04, ...,\n","          1.5907059e-03, -8.7412307e-04, -1.8088885e-03],\n","        [-3.2248036e-03,  2.7453382e-03, -3.3686825e-04, ...,\n","          1.9244031e-03, -3.0822551e-04, -1.9434711e-03],\n","        [-3.5003170e-03,  2.4040220e-03, -8.0947182e-04, ...,\n","          1.8368848e-03,  7.4745163e-05, -2.0496498e-03]],\n","\n","       [[-6.5123494e-04, -1.6956308e-04, -4.7167705e-04, ...,\n","          4.9701188e-04, -2.0004326e-04,  1.2820296e-04],\n","        [-1.0265603e-03,  8.6048130e-05, -8.5957098e-04, ...,\n","          4.8899325e-04, -2.2369489e-04,  3.7198217e-04],\n","        [-1.0044294e-03, -5.9709081e-04, -8.6828996e-04, ...,\n","          1.5636820e-04,  1.2489103e-04, -8.9310801e-05],\n","        ...,\n","        [ 1.1103537e-03, -2.1985794e-05, -3.1249824e-03, ...,\n","         -2.0774938e-03,  2.0654723e-03, -8.7015907e-04],\n","        [ 1.6203600e-03, -4.1660876e-04, -3.3119379e-03, ...,\n","         -2.7362611e-03,  2.8979990e-03, -1.6851688e-03],\n","        [ 2.0943230e-03, -9.1018248e-04, -3.4727019e-03, ...,\n","         -3.4055295e-03,  3.7223187e-03, -2.5173621e-03]],\n","\n","       [[-6.5123494e-04, -1.6956308e-04, -4.7167705e-04, ...,\n","          4.9701188e-04, -2.0004326e-04,  1.2820296e-04],\n","        [-1.4204110e-03,  6.9772948e-05, -2.6342177e-04, ...,\n","          5.1448221e-04, -9.9386857e-04,  5.4799090e-04],\n","        [-2.1480904e-03, -2.9739228e-04, -1.7535474e-04, ...,\n","         -1.9920514e-04, -1.8643407e-03,  1.0157865e-03],\n","        ...,\n","        [ 6.9412531e-04, -1.1235462e-05, -2.6916529e-03, ...,\n","         -2.8772154e-03,  1.0791752e-03,  2.1091381e-03],\n","        [ 1.5472308e-03, -6.2897359e-04, -2.8984668e-03, ...,\n","         -3.5631519e-03,  1.9473709e-03,  1.3535436e-03],\n","        [ 2.2751628e-03, -1.2831904e-03, -3.1202885e-03, ...,\n","         -4.2257626e-03,  2.7965747e-03,  4.6295315e-04]]], dtype=float32)>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sc3DZdwNxwC","executionInfo":{"status":"ok","timestamp":1660009845755,"user_tz":-540,"elapsed":23,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"93d0eede-6209-4f06-8023-11111383353f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"text_generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  24578048  \n","                                                                 \n"," lstm (LSTM)                 multiple                  33562624  \n","                                                                 \n"," lstm_1 (LSTM)               multiple                  33562624  \n","                                                                 \n"," dense (Dense)               multiple                  24590049  \n","                                                                 \n","=================================================================\n","Total params: 116,293,345\n","Trainable params: 116,293,345\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["- 모델 학습"],"metadata":{"id":"nKYnUTx1N8ez"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lh7sqOJ3-EI5","executionInfo":{"status":"ok","timestamp":1660011193884,"user_tz":-540,"elapsed":1348137,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"8cb786c8-e613-4709-cc27-5dc534998a54"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","4400/4400 [==============================] - 333s 75ms/step - loss: 2.8799 - val_loss: 2.5925\n","Epoch 2/4\n","4400/4400 [==============================] - 327s 74ms/step - loss: 2.3236 - val_loss: 2.3523\n","Epoch 3/4\n","4400/4400 [==============================] - 327s 74ms/step - loss: 1.9010 - val_loss: 2.2387\n","Epoch 4/4\n","4400/4400 [==============================] - 327s 74ms/step - loss: 1.5734 - val_loss: 2.1981\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fcbf96bfbd0>"]},"metadata":{},"execution_count":15}],"source":["optimizer = tf.keras.optimizers.Adam()\n","#Loss\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","\n","model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val), epochs=4)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"68PwPnFnASN_","executionInfo":{"status":"ok","timestamp":1660011193885,"user_tz":-540,"elapsed":15,"user":{"displayName":"곽민기","userId":"11982204351407286297"}}},"outputs":[],"source":["def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 단어 하나씩 예측해 문장을 만듭니다\n","    #    1. 입력받은 문장의 텐서를 입력합니다\n","    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n","    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n","    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n","    while True:\n","        # 1\n","        predict = model(test_tensor) \n","        # 2\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n","        # 3 \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        # 4\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"baGj3MVCiO6K","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1660011193885,"user_tz":-540,"elapsed":14,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"f2c81805-fe0c-49ca-8d76-cc3bbeacb412"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> i love you so much , i love you so much <end> '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["generate_text(model, tokenizer, init_sentence=\"<start> i love\")"]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> i hate\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Ga5R5vcQOD6y","executionInfo":{"status":"ok","timestamp":1660011193886,"user_tz":-540,"elapsed":13,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"ddb8ba62-4e87-4b0c-c81e-7b11412548ad"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> i hate to see her leave sometimes <end> '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":19,"metadata":{"id":"MPExdzhQiUaq","executionInfo":{"status":"ok","timestamp":1660011194396,"user_tz":-540,"elapsed":522,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"8dc15120-86ae-4ba6-e5d0-eab956b9d970"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> i have been here with you <end> '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["generate_text(model, tokenizer, init_sentence=\"<start> i have\")"]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> what is\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"p8U885S0OJM7","executionInfo":{"status":"ok","timestamp":1660011194398,"user_tz":-540,"elapsed":12,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"0e1368f7-c19e-48c6-eee7-308fcca0f911"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> what is the answer , what are you waiting for ? <end> '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> why so\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"WC8h7E5nOJgR","executionInfo":{"status":"ok","timestamp":1660011194399,"user_tz":-540,"elapsed":12,"user":{"displayName":"곽민기","userId":"11982204351407286297"}},"outputId":"d4435e81-913e-4695-e753-e8bae47bbf66"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> why so ray and aunt <unk> always <unk> <end> '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["진행 과정\n","  - 데이터 불러오기\n","  - 데이터 정제\n","    - 조건 중 '토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기' 라는 조건이 있었음.\n","    - 저 조건을 만족하기 위해 for sentence in raw_corpus: 에서 if len(sentence) >= 15 continue 를 사용\n","    - 하지만 데이터 셋이 뭔가 이상해짐\n","    - 그래서 맨 처음 데이터를 불러오고 raw_corpus를 raw_corpus[:15]로 만듦\n","    - 데이터 갯수가 너무 줄어듬\n","  - 평가 데이터 분리\n","    - 위에서 실패한 조건을 여기서 시도\n","    - tensor 변환할 때 maxlen = 15를 줌\n","    - 데이터 형태가 (~~:14)가 나옴\n","    - maxlen = 16으로 변경\n","    - 데이터 형태 (~~:15)\n","  - 모델 학습 준비\n","    - embedding_size, hidden_size 를 128, 256, 512, 1024, 2048 로 변경해가면 진행\n","    - epoch는 7로 진행\n","    - val_loss = 2.27 까지 도달\n","    - 2.2 이하로 내려야됨\n","    - 2048, 2048에서 4/7에서 2.1 도달 \n","    - 하지만 5/7에서 오버피팅 \n","    - epoch = 4로 진행 \n","    - val_loss = 2.19로 성공\n","  - 결과\n","    - i love -> <start> i love you so much , i love you so much <end> \n","    - i hate -> <start> i hate to see her leave sometimes <end> \n","    - i have -> <start> i have been here with you <end> \n","    - what is -> <start> what is the answer , what are you waiting for ? <end> \n","    - why so -> <start> why so ray and aunt <unk> always <unk> <end> \n","  - 다른 시도 \n","    - 위의 조건은 권고 사항이므로 루브릭 기준엔 없음\n","    - 그래서 maxlen을 20으로 두고 진행\n","    - 큰 어려움 없이 val_loss 2.2 이하로 내려감\n"],"metadata":{"id":"oxTsD_jSOQdR"}},{"cell_type":"markdown","source":["### 회고\n","- 조건을 만족하면서 val_loss를 2.2 이하로 내리는게 힘들었음\n","- 임베딩 사이즈와 히든 사이즈를 변경해가며 학습시킨 결과 사이즈가 크면 오버피팅이 잘나고 작으면 val_loss가 잘 안내려감"],"metadata":{"id":"Kv4cY5mFSTUf"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"[E-06] lyrics AI .ipynb","provenance":[],"mount_file_id":"15S7v0LQaY51-ZzHiCpP12ovolBSGBS1k","authorship_tag":"ABX9TyNxT5UeSbbpkWkz48N1IhDP"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}